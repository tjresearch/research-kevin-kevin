{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  3\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "#needed for jupyter notebook\n",
    "#https://github.com/tensorflow/tensorflow/issues/24828\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "\n",
    "# https://github.com/jkjung-avt/keras-cats-dogs-tutorial/blob/master/train_resnet50.py\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "hyperparameters + filenames\n",
    "\"\"\"\n",
    "DATASET_PATH = \"data\"\n",
    "\n",
    "# doubled min width in dataset\n",
    "# height = 2*width\n",
    "# height, width \n",
    "# try reducing batch size or freeze more layers if your GPU runs out of memory\n",
    "\n",
    "TARGET_SIZE = (224,112) \n",
    "BATCH_SIZE = 64  \n",
    "NUM_EPOCHS = 200\n",
    "# NUM_CLASSES defined later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 622 images belonging to 13 classes.\n",
      "Found 152 images belonging to 13 classes.\n",
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            [(None, 224, 112, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 230, 118, 3)  0           input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 112, 56, 64)  9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 112, 56, 64)  256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 112, 56, 64)  0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 114, 58, 64)  0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 56, 28, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 56, 28, 64)   4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 56, 28, 64)   256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 56, 28, 64)   0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 56, 28, 64)   36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 56, 28, 64)   256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 56, 28, 64)   0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 56, 28, 256)  16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 56, 28, 256)  16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 56, 28, 256)  1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 56, 28, 256)  1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 56, 28, 256)  0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 56, 28, 256)  0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 56, 28, 64)   16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 56, 28, 64)   256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 56, 28, 64)   0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 56, 28, 64)   36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 56, 28, 64)   256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 56, 28, 64)   0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 56, 28, 256)  16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 56, 28, 256)  1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 56, 28, 256)  0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 56, 28, 256)  0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 56, 28, 64)   16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 56, 28, 64)   256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 56, 28, 64)   0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 56, 28, 64)   36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 56, 28, 64)   256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 56, 28, 64)   0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 56, 28, 256)  16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 56, 28, 256)  1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 56, 28, 256)  0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 56, 28, 256)  0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 28, 14, 128)  32896       conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 28, 14, 128)  512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 28, 14, 128)  0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 28, 14, 128)  147584      conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 28, 14, 128)  512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 28, 14, 128)  0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 28, 14, 512)  131584      conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 28, 14, 512)  66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 28, 14, 512)  2048        conv3_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_bn (BatchNormali (None, 28, 14, 512)  2048        conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_add (Add)          (None, 28, 14, 512)  0           conv3_block1_0_bn[0][0]          \n",
      "                                                                 conv3_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Activation)   (None, 28, 14, 512)  0           conv3_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 28, 14, 128)  65664       conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 28, 14, 128)  512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 28, 14, 128)  0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 28, 14, 128)  147584      conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 28, 14, 128)  512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 28, 14, 128)  0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 28, 14, 512)  66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_bn (BatchNormali (None, 28, 14, 512)  2048        conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_add (Add)          (None, 28, 14, 512)  0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Activation)   (None, 28, 14, 512)  0           conv3_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 28, 14, 128)  65664       conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 28, 14, 128)  512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 28, 14, 128)  0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 28, 14, 128)  147584      conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, 28, 14, 128)  512         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 28, 14, 128)  0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 28, 14, 512)  66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_bn (BatchNormali (None, 28, 14, 512)  2048        conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_add (Add)          (None, 28, 14, 512)  0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Activation)   (None, 28, 14, 512)  0           conv3_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 28, 14, 128)  65664       conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 28, 14, 128)  512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 28, 14, 128)  0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 28, 14, 128)  147584      conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, 28, 14, 128)  512         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 28, 14, 128)  0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 28, 14, 512)  66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_bn (BatchNormali (None, 28, 14, 512)  2048        conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_add (Add)          (None, 28, 14, 512)  0           conv3_block3_out[0][0]           \n",
      "                                                                 conv3_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Activation)   (None, 28, 14, 512)  0           conv3_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 14, 7, 256)   131328      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 14, 7, 256)   1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 14, 7, 256)   0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 14, 7, 256)   590080      conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 14, 7, 256)   1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 14, 7, 256)   0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 14, 7, 1024)  525312      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 14, 7, 1024)  263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 14, 7, 1024)  4096        conv4_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_bn (BatchNormali (None, 14, 7, 1024)  4096        conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_add (Add)          (None, 14, 7, 1024)  0           conv4_block1_0_bn[0][0]          \n",
      "                                                                 conv4_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Activation)   (None, 14, 7, 1024)  0           conv4_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 14, 7, 256)   262400      conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 14, 7, 256)   1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 14, 7, 256)   0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 14, 7, 256)   590080      conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 14, 7, 256)   1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 14, 7, 256)   0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 14, 7, 1024)  263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_bn (BatchNormali (None, 14, 7, 1024)  4096        conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_add (Add)          (None, 14, 7, 1024)  0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Activation)   (None, 14, 7, 1024)  0           conv4_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 14, 7, 256)   262400      conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 14, 7, 256)   1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 14, 7, 256)   0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 14, 7, 256)   590080      conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, 14, 7, 256)   1024        conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 14, 7, 256)   0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 14, 7, 1024)  263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_bn (BatchNormali (None, 14, 7, 1024)  4096        conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_add (Add)          (None, 14, 7, 1024)  0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Activation)   (None, 14, 7, 1024)  0           conv4_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 14, 7, 256)   262400      conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 14, 7, 256)   1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 14, 7, 256)   0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 14, 7, 256)   590080      conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, 14, 7, 256)   1024        conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 14, 7, 256)   0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 14, 7, 1024)  263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_bn (BatchNormali (None, 14, 7, 1024)  4096        conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_add (Add)          (None, 14, 7, 1024)  0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Activation)   (None, 14, 7, 1024)  0           conv4_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 14, 7, 256)   262400      conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 14, 7, 256)   1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 14, 7, 256)   0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 14, 7, 256)   590080      conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, 14, 7, 256)   1024        conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 14, 7, 256)   0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 14, 7, 1024)  263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_bn (BatchNormali (None, 14, 7, 1024)  4096        conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_add (Add)          (None, 14, 7, 1024)  0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Activation)   (None, 14, 7, 1024)  0           conv4_block5_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 14, 7, 256)   262400      conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 14, 7, 256)   1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 14, 7, 256)   0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 14, 7, 256)   590080      conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, 14, 7, 256)   1024        conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 14, 7, 256)   0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 14, 7, 1024)  263168      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_bn (BatchNormali (None, 14, 7, 1024)  4096        conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_add (Add)          (None, 14, 7, 1024)  0           conv4_block5_out[0][0]           \n",
      "                                                                 conv4_block6_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Activation)   (None, 14, 7, 1024)  0           conv4_block6_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 7, 4, 512)    524800      conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 7, 4, 512)    2048        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 7, 4, 512)    0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 7, 4, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, 7, 4, 512)    2048        conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, 7, 4, 512)    0           conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 7, 4, 2048)   2099200     conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, 7, 4, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 7, 4, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_bn (BatchNormali (None, 7, 4, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_add (Add)          (None, 7, 4, 2048)   0           conv5_block1_0_bn[0][0]          \n",
      "                                                                 conv5_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Activation)   (None, 7, 4, 2048)   0           conv5_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 7, 4, 512)    1049088     conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 7, 4, 512)    2048        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 7, 4, 512)    0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 7, 4, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, 7, 4, 512)    2048        conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, 7, 4, 512)    0           conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, 7, 4, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_bn (BatchNormali (None, 7, 4, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_add (Add)          (None, 7, 4, 2048)   0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Activation)   (None, 7, 4, 2048)   0           conv5_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 7, 4, 512)    1049088     conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 7, 4, 512)    2048        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 7, 4, 512)    0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 7, 4, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_bn (BatchNormali (None, 7, 4, 512)    2048        conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, 7, 4, 512)    0           conv5_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, 7, 4, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_bn (BatchNormali (None, 7, 4, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_add (Add)          (None, 7, 4, 2048)   0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Activation)   (None, 7, 4, 2048)   0           conv5_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 57344)        0           conv5_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 57344)        0           flatten_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "softmax (Dense)                 (None, 13)           745485      dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 24,333,197\n",
      "Trainable params: 24,280,077\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "8/9 [=========================>....] - ETA: 1s - loss: 4.7946 - acc: 0.1074Epoch 1/200\n",
      "9/9 [==============================] - 13s 1s/step - loss: 4.7766 - acc: 0.1147 - val_loss: 3.5454 - val_acc: 0.1719\n",
      "Epoch 2/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 4.0449 - acc: 0.1992Epoch 1/200\n",
      "9/9 [==============================] - 3s 359ms/step - loss: 4.0189 - acc: 0.1997 - val_loss: 3.3939 - val_acc: 0.2891\n",
      "Epoch 3/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 4.1482 - acc: 0.2247Epoch 1/200\n",
      "9/9 [==============================] - 3s 332ms/step - loss: 4.0501 - acc: 0.2401 - val_loss: 3.2354 - val_acc: 0.3125\n",
      "Epoch 4/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 3.9068 - acc: 0.2710Epoch 1/200\n",
      "9/9 [==============================] - 3s 331ms/step - loss: 3.8366 - acc: 0.2833 - val_loss: 3.0263 - val_acc: 0.3125\n",
      "Epoch 5/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 3.7510 - acc: 0.2812Epoch 1/200\n",
      "9/9 [==============================] - 3s 316ms/step - loss: 3.7270 - acc: 0.2832 - val_loss: 2.8086 - val_acc: 0.3281\n",
      "Epoch 6/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 3.2995 - acc: 0.3320Epoch 1/200\n",
      "9/9 [==============================] - 3s 322ms/step - loss: 3.3776 - acc: 0.3194 - val_loss: 2.6629 - val_acc: 0.3594\n",
      "Epoch 7/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 3.2618 - acc: 0.3320Epoch 1/200\n",
      "9/9 [==============================] - 3s 321ms/step - loss: 3.3285 - acc: 0.3208 - val_loss: 2.5745 - val_acc: 0.3672\n",
      "Epoch 8/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 3.1860 - acc: 0.3381Epoch 1/200\n",
      "9/9 [==============================] - 3s 333ms/step - loss: 3.1080 - acc: 0.3423 - val_loss: 2.4607 - val_acc: 0.3984\n",
      "Epoch 9/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 3.1815 - acc: 0.3340Epoch 1/200\n",
      "9/9 [==============================] - 3s 343ms/step - loss: 3.1085 - acc: 0.3423 - val_loss: 2.3268 - val_acc: 0.4219\n",
      "Epoch 10/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 3.0529 - acc: 0.3360Epoch 1/200\n",
      "9/9 [==============================] - 3s 312ms/step - loss: 3.0095 - acc: 0.3423 - val_loss: 2.2119 - val_acc: 0.4453\n",
      "Epoch 11/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 2.9243 - acc: 0.3846Epoch 1/200\n",
      "9/9 [==============================] - 3s 324ms/step - loss: 2.9149 - acc: 0.3871 - val_loss: 2.0814 - val_acc: 0.4766\n",
      "Epoch 12/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 3.3405 - acc: 0.3457Epoch 1/200\n",
      "9/9 [==============================] - 3s 329ms/step - loss: 3.2230 - acc: 0.3548 - val_loss: 1.9828 - val_acc: 0.4766\n",
      "Epoch 13/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 2.8317 - acc: 0.3887Epoch 1/200\n",
      "9/9 [==============================] - 3s 324ms/step - loss: 2.7793 - acc: 0.3925 - val_loss: 1.9243 - val_acc: 0.4922\n",
      "Epoch 14/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 2.7552 - acc: 0.4130Epoch 1/200\n",
      "9/9 [==============================] - 3s 325ms/step - loss: 2.6943 - acc: 0.4122 - val_loss: 1.8973 - val_acc: 0.5312\n",
      "Epoch 15/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 2.5273 - acc: 0.4395Epoch 1/200\n",
      "9/9 [==============================] - 3s 344ms/step - loss: 2.5692 - acc: 0.4358 - val_loss: 1.8381 - val_acc: 0.5547\n",
      "Epoch 16/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 2.7215 - acc: 0.3866Epoch 1/200\n",
      "9/9 [==============================] - 3s 310ms/step - loss: 2.7022 - acc: 0.3925 - val_loss: 1.7943 - val_acc: 0.5781\n",
      "Epoch 17/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 2.5290 - acc: 0.4352Epoch 1/200\n",
      "9/9 [==============================] - 3s 328ms/step - loss: 2.5375 - acc: 0.4319 - val_loss: 1.7751 - val_acc: 0.5859\n",
      "Epoch 18/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 2.5694 - acc: 0.4514Epoch 1/200\n",
      "9/9 [==============================] - 3s 323ms/step - loss: 2.5881 - acc: 0.4552 - val_loss: 1.7411 - val_acc: 0.5859\n",
      "Epoch 19/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 2.4333 - acc: 0.4393Epoch 1/200\n",
      "9/9 [==============================] - 3s 330ms/step - loss: 2.4800 - acc: 0.4301 - val_loss: 1.6656 - val_acc: 0.5859\n",
      "Epoch 20/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 2.4243 - acc: 0.4676Epoch 1/200\n",
      "9/9 [==============================] - 3s 324ms/step - loss: 2.4967 - acc: 0.4588 - val_loss: 1.5988 - val_acc: 0.5938\n",
      "Epoch 21/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 2.2869 - acc: 0.4777Epoch 1/200\n",
      "9/9 [==============================] - 3s 337ms/step - loss: 2.3050 - acc: 0.4713 - val_loss: 1.5299 - val_acc: 0.6172\n",
      "Epoch 22/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 2.2380 - acc: 0.4858Epoch 1/200\n",
      "9/9 [==============================] - 3s 343ms/step - loss: 2.2166 - acc: 0.4875 - val_loss: 1.5036 - val_acc: 0.6250\n",
      "Epoch 23/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 2.2194 - acc: 0.4676Epoch 1/200\n",
      "9/9 [==============================] - 3s 336ms/step - loss: 2.1873 - acc: 0.4767 - val_loss: 1.4971 - val_acc: 0.6172\n",
      "Epoch 24/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 2.0029 - acc: 0.4980Epoch 1/200\n",
      "9/9 [==============================] - 3s 341ms/step - loss: 2.0958 - acc: 0.4861 - val_loss: 1.4751 - val_acc: 0.6172\n",
      "Epoch 25/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 2.2909 - acc: 0.4769Epoch 1/200\n",
      "9/9 [==============================] - 3s 323ms/step - loss: 2.3635 - acc: 0.4630 - val_loss: 1.5212 - val_acc: 0.6094\n",
      "Epoch 26/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 2.1004 - acc: 0.4960Epoch 1/200\n",
      "9/9 [==============================] - 3s 322ms/step - loss: 2.1526 - acc: 0.4839 - val_loss: 1.5025 - val_acc: 0.6172\n",
      "Epoch 27/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 1.9483 - acc: 0.5215Epoch 1/200\n",
      "9/9 [==============================] - 3s 341ms/step - loss: 1.9283 - acc: 0.5226 - val_loss: 1.4332 - val_acc: 0.6250\n",
      "Epoch 28/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 2.0117 - acc: 0.5202Epoch 1/200\n",
      "9/9 [==============================] - 3s 319ms/step - loss: 2.0363 - acc: 0.5197 - val_loss: 1.3792 - val_acc: 0.6250\n",
      "Epoch 29/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 1.9635 - acc: 0.5385Epoch 1/200\n",
      "9/9 [==============================] - 3s 303ms/step - loss: 1.9863 - acc: 0.5333 - val_loss: 1.3146 - val_acc: 0.6250\n",
      "Epoch 30/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 1.9340 - acc: 0.5371Epoch 1/200\n",
      "9/9 [==============================] - 3s 350ms/step - loss: 1.9429 - acc: 0.5312 - val_loss: 1.2360 - val_acc: 0.6328\n",
      "Epoch 31/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 2.0516 - acc: 0.5101Epoch 1/200\n",
      "9/9 [==============================] - 3s 319ms/step - loss: 1.9919 - acc: 0.5287 - val_loss: 1.1765 - val_acc: 0.6172\n",
      "Epoch 32/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 1.6655 - acc: 0.5769Epoch 1/200\n",
      "9/9 [==============================] - 3s 327ms/step - loss: 1.6540 - acc: 0.5771 - val_loss: 1.1557 - val_acc: 0.6562\n",
      "Epoch 33/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 2.1602 - acc: 0.5081Epoch 1/200\n",
      "9/9 [==============================] - 3s 337ms/step - loss: 2.0699 - acc: 0.5125 - val_loss: 1.1640 - val_acc: 0.6562\n",
      "Epoch 34/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 1.8163 - acc: 0.5526Epoch 1/200\n",
      "9/9 [==============================] - 3s 318ms/step - loss: 1.7980 - acc: 0.5520 - val_loss: 1.1438 - val_acc: 0.6641\n",
      "Epoch 35/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 1.9682 - acc: 0.4980Epoch 1/200\n",
      "9/9 [==============================] - 3s 333ms/step - loss: 1.9443 - acc: 0.4964 - val_loss: 1.1250 - val_acc: 0.6562\n",
      "Epoch 36/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 1.7070 - acc: 0.5859Epoch 1/200\n",
      "9/9 [==============================] - 3s 340ms/step - loss: 1.7116 - acc: 0.5851 - val_loss: 1.1156 - val_acc: 0.6719\n",
      "Epoch 37/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 1.8333 - acc: 0.5526Epoch 1/200\n",
      "9/9 [==============================] - 3s 320ms/step - loss: 1.8085 - acc: 0.5502 - val_loss: 1.1050 - val_acc: 0.6797\n",
      "Epoch 38/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 1.6208 - acc: 0.5891Epoch 1/200\n",
      "9/9 [==============================] - 3s 340ms/step - loss: 1.6644 - acc: 0.5789 - val_loss: 1.0638 - val_acc: 0.6953\n",
      "Epoch 39/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 1.7666 - acc: 0.5628Epoch 1/200\n",
      "9/9 [==============================] - 3s 315ms/step - loss: 1.7065 - acc: 0.5722 - val_loss: 1.0363 - val_acc: 0.7188\n",
      "Epoch 40/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 1.5452 - acc: 0.5625Epoch 1/200\n",
      "9/9 [==============================] - 3s 334ms/step - loss: 1.5707 - acc: 0.5590 - val_loss: 1.0110 - val_acc: 0.7344\n",
      "Epoch 41/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 1.4622 - acc: 0.6012Epoch 1/200\n",
      "9/9 [==============================] - 3s 324ms/step - loss: 1.5406 - acc: 0.5932 - val_loss: 1.0022 - val_acc: 0.7266\n",
      "Epoch 42/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 1.7115 - acc: 0.5911Epoch 1/200\n",
      "9/9 [==============================] - 3s 338ms/step - loss: 1.7111 - acc: 0.5932 - val_loss: 0.9814 - val_acc: 0.7344\n",
      "Epoch 43/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 1.4770 - acc: 0.5931Epoch 1/200\n",
      "9/9 [==============================] - 3s 342ms/step - loss: 1.4972 - acc: 0.5914 - val_loss: 0.9729 - val_acc: 0.7422\n",
      "Epoch 44/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 1.7095 - acc: 0.5850Epoch 1/200\n",
      "9/9 [==============================] - 3s 325ms/step - loss: 1.6751 - acc: 0.5824 - val_loss: 0.9624 - val_acc: 0.7500\n",
      "Epoch 45/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 1.5233 - acc: 0.5977Epoch 1/200\n",
      "9/9 [==============================] - 3s 337ms/step - loss: 1.5343 - acc: 0.5972 - val_loss: 0.9571 - val_acc: 0.7422\n",
      "Epoch 46/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 1.6018 - acc: 0.6134Epoch 1/200\n",
      "9/9 [==============================] - 3s 320ms/step - loss: 1.5784 - acc: 0.6130 - val_loss: 0.9586 - val_acc: 0.7344\n",
      "Epoch 47/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 1.5803 - acc: 0.5850Epoch 1/200\n",
      "9/9 [==============================] - 3s 335ms/step - loss: 1.5802 - acc: 0.5878 - val_loss: 0.9483 - val_acc: 0.7344\n",
      "Epoch 48/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 1.5362 - acc: 0.5951Epoch 1/200\n",
      "9/9 [==============================] - 3s 329ms/step - loss: 1.5266 - acc: 0.5950 - val_loss: 0.9384 - val_acc: 0.7500\n",
      "Epoch 49/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 1.3277 - acc: 0.6309Epoch 1/200\n",
      "9/9 [==============================] - 3s 329ms/step - loss: 1.2999 - acc: 0.6372 - val_loss: 0.9102 - val_acc: 0.7344\n",
      "Epoch 50/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 1.3838 - acc: 0.6309Epoch 1/200\n",
      "9/9 [==============================] - 3s 328ms/step - loss: 1.4294 - acc: 0.6237 - val_loss: 0.8733 - val_acc: 0.7578\n",
      "Epoch 51/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 1.3886 - acc: 0.6215Epoch 1/200\n",
      "9/9 [==============================] - 3s 341ms/step - loss: 1.3678 - acc: 0.6254 - val_loss: 0.8543 - val_acc: 0.7500\n",
      "Epoch 52/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 1.2393 - acc: 0.6619Epoch 1/200\n",
      "9/9 [==============================] - 3s 339ms/step - loss: 1.2382 - acc: 0.6595 - val_loss: 0.8396 - val_acc: 0.7578\n",
      "Epoch 53/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 1.4724 - acc: 0.6348Epoch 1/200\n",
      "9/9 [==============================] - 3s 341ms/step - loss: 1.4433 - acc: 0.6389 - val_loss: 0.8375 - val_acc: 0.7500\n",
      "Epoch 54/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 1.2993 - acc: 0.6429Epoch 1/200\n",
      "9/9 [==============================] - 3s 328ms/step - loss: 1.2901 - acc: 0.6444 - val_loss: 0.8292 - val_acc: 0.7578\n",
      "Epoch 55/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 1.2802 - acc: 0.6457Epoch 1/200\n",
      "9/9 [==============================] - 3s 323ms/step - loss: 1.2987 - acc: 0.6505 - val_loss: 0.8146 - val_acc: 0.7656\n",
      "Epoch 56/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 1.2024 - acc: 0.6699Epoch 1/200\n",
      "9/9 [==============================] - 3s 318ms/step - loss: 1.2686 - acc: 0.6649 - val_loss: 0.8187 - val_acc: 0.7500\n",
      "Epoch 57/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 1.3666 - acc: 0.6289Epoch 1/200\n",
      "9/9 [==============================] - 3s 328ms/step - loss: 1.4021 - acc: 0.6233 - val_loss: 0.8155 - val_acc: 0.7500\n",
      "Epoch 58/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 1.2070 - acc: 0.6680Epoch 1/200\n",
      "9/9 [==============================] - 3s 334ms/step - loss: 1.2474 - acc: 0.6595 - val_loss: 0.7939 - val_acc: 0.7734\n",
      "Epoch 59/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 1.1251 - acc: 0.6781Epoch 1/200\n",
      "9/9 [==============================] - 3s 317ms/step - loss: 1.1600 - acc: 0.6720 - val_loss: 0.7746 - val_acc: 0.7812\n",
      "Epoch 60/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 1.3001 - acc: 0.6445Epoch 1/200\n",
      "9/9 [==============================] - 3s 312ms/step - loss: 1.2791 - acc: 0.6541 - val_loss: 0.7725 - val_acc: 0.7812\n",
      "Epoch 61/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 1.0789 - acc: 0.7031Epoch 1/200\n",
      "9/9 [==============================] - 3s 337ms/step - loss: 1.1183 - acc: 0.7014 - val_loss: 0.7617 - val_acc: 0.7734\n",
      "Epoch 62/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 1.1153 - acc: 0.6765Epoch 1/200\n",
      "9/9 [==============================] - 3s 353ms/step - loss: 1.1161 - acc: 0.6796 - val_loss: 0.7495 - val_acc: 0.7812\n",
      "Epoch 63/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 1.1338 - acc: 0.6862Epoch 1/200\n",
      "9/9 [==============================] - 3s 330ms/step - loss: 1.1133 - acc: 0.6989 - val_loss: 0.7510 - val_acc: 0.7891\n",
      "Epoch 64/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 1.1288 - acc: 0.6836Epoch 1/200\n",
      "9/9 [==============================] - 3s 349ms/step - loss: 1.1461 - acc: 0.6788 - val_loss: 0.7625 - val_acc: 0.7969\n",
      "Epoch 65/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 1.1324 - acc: 0.6828Epoch 1/200\n",
      "9/9 [==============================] - 3s 318ms/step - loss: 1.1527 - acc: 0.6815 - val_loss: 0.7684 - val_acc: 0.7891\n",
      "Epoch 66/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 1.0611 - acc: 0.7146Epoch 1/200\n",
      "9/9 [==============================] - 3s 312ms/step - loss: 1.0519 - acc: 0.7204 - val_loss: 0.7710 - val_acc: 0.7812\n",
      "Epoch 67/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 1.2018 - acc: 0.6761Epoch 1/200\n",
      "9/9 [==============================] - 3s 321ms/step - loss: 1.2203 - acc: 0.6703 - val_loss: 0.7606 - val_acc: 0.7891\n",
      "Epoch 68/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 1.2807 - acc: 0.6719Epoch 1/200\n",
      "9/9 [==============================] - 3s 349ms/step - loss: 1.2590 - acc: 0.6719 - val_loss: 0.7435 - val_acc: 0.7891\n",
      "Epoch 69/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 1.1180 - acc: 0.6822Epoch 1/200\n",
      "9/9 [==============================] - 3s 309ms/step - loss: 1.0856 - acc: 0.6882 - val_loss: 0.7391 - val_acc: 0.7891\n",
      "Epoch 70/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 1.0804 - acc: 0.7024Epoch 1/200\n",
      "9/9 [==============================] - 3s 333ms/step - loss: 1.0605 - acc: 0.7025 - val_loss: 0.7375 - val_acc: 0.7891\n",
      "Epoch 71/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 1.1700 - acc: 0.7070Epoch 1/200\n",
      "9/9 [==============================] - 3s 331ms/step - loss: 1.1481 - acc: 0.6997 - val_loss: 0.7387 - val_acc: 0.7891\n",
      "Epoch 72/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 1.0273 - acc: 0.6870Epoch 1/200\n",
      "9/9 [==============================] - 3s 336ms/step - loss: 1.0363 - acc: 0.6926 - val_loss: 0.7128 - val_acc: 0.7891\n",
      "Epoch 73/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 1.0341 - acc: 0.7207Epoch 1/200\n",
      "9/9 [==============================] - 3s 352ms/step - loss: 0.9900 - acc: 0.7274 - val_loss: 0.6910 - val_acc: 0.7891\n",
      "Epoch 74/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 1.1553 - acc: 0.6964Epoch 1/200\n",
      "9/9 [==============================] - 3s 339ms/step - loss: 1.0909 - acc: 0.7097 - val_loss: 0.6884 - val_acc: 0.7891\n",
      "Epoch 75/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 1.0341 - acc: 0.6996Epoch 1/200\n",
      "9/9 [==============================] - 3s 313ms/step - loss: 1.0537 - acc: 0.7000 - val_loss: 0.6902 - val_acc: 0.7891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 1.1422 - acc: 0.6741Epoch 1/200\n",
      "9/9 [==============================] - 3s 329ms/step - loss: 1.1385 - acc: 0.6774 - val_loss: 0.6979 - val_acc: 0.7734\n",
      "Epoch 77/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 1.1805 - acc: 0.7051Epoch 1/200\n",
      "9/9 [==============================] - 3s 341ms/step - loss: 1.1503 - acc: 0.7101 - val_loss: 0.7058 - val_acc: 0.7812\n",
      "Epoch 78/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 1.0374 - acc: 0.7045Epoch 1/200\n",
      "9/9 [==============================] - 3s 333ms/step - loss: 1.0068 - acc: 0.7043 - val_loss: 0.7011 - val_acc: 0.7891\n",
      "Epoch 79/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 1.0289 - acc: 0.7045Epoch 1/200\n",
      "9/9 [==============================] - 3s 330ms/step - loss: 0.9996 - acc: 0.7079 - val_loss: 0.6717 - val_acc: 0.7812\n",
      "Epoch 80/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 1.0163 - acc: 0.7126Epoch 1/200\n",
      "9/9 [==============================] - 3s 311ms/step - loss: 0.9808 - acc: 0.7186 - val_loss: 0.6517 - val_acc: 0.7891\n",
      "Epoch 81/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.8855 - acc: 0.7348Epoch 1/200\n",
      "9/9 [==============================] - 3s 335ms/step - loss: 0.9148 - acc: 0.7276 - val_loss: 0.6327 - val_acc: 0.7891\n",
      "Epoch 82/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.9930 - acc: 0.7188Epoch 1/200\n",
      "9/9 [==============================] - 3s 341ms/step - loss: 1.0271 - acc: 0.7205 - val_loss: 0.6238 - val_acc: 0.7891\n",
      "Epoch 83/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.9265 - acc: 0.7521Epoch 1/200\n",
      "9/9 [==============================] - 3s 339ms/step - loss: 0.8728 - acc: 0.7611 - val_loss: 0.6084 - val_acc: 0.8047\n",
      "Epoch 84/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 1.0172 - acc: 0.7070Epoch 1/200\n",
      "9/9 [==============================] - 3s 344ms/step - loss: 1.0083 - acc: 0.7083 - val_loss: 0.6036 - val_acc: 0.8047\n",
      "Epoch 85/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.8846 - acc: 0.7287Epoch 1/200\n",
      "9/9 [==============================] - 3s 328ms/step - loss: 0.9131 - acc: 0.7312 - val_loss: 0.5964 - val_acc: 0.7969\n",
      "Epoch 86/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 1.0045 - acc: 0.7287Epoch 1/200\n",
      "9/9 [==============================] - 3s 321ms/step - loss: 0.9686 - acc: 0.7294 - val_loss: 0.5869 - val_acc: 0.7969\n",
      "Epoch 87/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.8127 - acc: 0.7652Epoch 1/200\n",
      "9/9 [==============================] - 3s 319ms/step - loss: 0.8235 - acc: 0.7581 - val_loss: 0.5861 - val_acc: 0.7969\n",
      "Epoch 88/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.8843 - acc: 0.7449Epoch 1/200\n",
      "9/9 [==============================] - 3s 342ms/step - loss: 0.8997 - acc: 0.7473 - val_loss: 0.5841 - val_acc: 0.7969\n",
      "Epoch 89/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.7954 - acc: 0.7591Epoch 1/200\n",
      "9/9 [==============================] - 3s 321ms/step - loss: 0.7916 - acc: 0.7581 - val_loss: 0.5846 - val_acc: 0.8047\n",
      "Epoch 90/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.9053 - acc: 0.7166Epoch 1/200\n",
      "9/9 [==============================] - 3s 334ms/step - loss: 0.8817 - acc: 0.7204 - val_loss: 0.5879 - val_acc: 0.8125\n",
      "Epoch 91/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.8595 - acc: 0.7348Epoch 1/200\n",
      "9/9 [==============================] - 3s 322ms/step - loss: 0.8404 - acc: 0.7366 - val_loss: 0.5876 - val_acc: 0.8203\n",
      "Epoch 92/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.8454 - acc: 0.7480Epoch 1/200\n",
      "9/9 [==============================] - 3s 345ms/step - loss: 0.8553 - acc: 0.7413 - val_loss: 0.5849 - val_acc: 0.8281\n",
      "Epoch 93/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 1.0009 - acc: 0.7085Epoch 1/200\n",
      "9/9 [==============================] - 3s 332ms/step - loss: 0.9635 - acc: 0.7186 - val_loss: 0.5817 - val_acc: 0.8203\n",
      "Epoch 94/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.6837 - acc: 0.8097Epoch 1/200\n",
      "9/9 [==============================] - 3s 310ms/step - loss: 0.7098 - acc: 0.8056 - val_loss: 0.5803 - val_acc: 0.8125\n",
      "Epoch 95/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.8198 - acc: 0.7578Epoch 1/200\n",
      "9/9 [==============================] - 3s 346ms/step - loss: 0.7955 - acc: 0.7587 - val_loss: 0.5782 - val_acc: 0.8047\n",
      "Epoch 96/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.8099 - acc: 0.7563Epoch 1/200\n",
      "9/9 [==============================] - 3s 316ms/step - loss: 0.7876 - acc: 0.7704 - val_loss: 0.5733 - val_acc: 0.8125\n",
      "Epoch 97/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.8762 - acc: 0.7578Epoch 1/200\n",
      "9/9 [==============================] - 3s 335ms/step - loss: 0.8682 - acc: 0.7622 - val_loss: 0.5605 - val_acc: 0.8125\n",
      "Epoch 98/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.8263 - acc: 0.7632Epoch 1/200\n",
      "9/9 [==============================] - 3s 331ms/step - loss: 0.8289 - acc: 0.7706 - val_loss: 0.5530 - val_acc: 0.8125\n",
      "Epoch 99/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.9363 - acc: 0.7227Epoch 1/200\n",
      "9/9 [==============================] - 3s 341ms/step - loss: 0.9212 - acc: 0.7240 - val_loss: 0.5469 - val_acc: 0.8125\n",
      "Epoch 100/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.7411 - acc: 0.7632Epoch 1/200\n",
      "9/9 [==============================] - 3s 311ms/step - loss: 0.7380 - acc: 0.7688 - val_loss: 0.5462 - val_acc: 0.8125\n",
      "Epoch 101/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.9798 - acc: 0.7429Epoch 1/200\n",
      "9/9 [==============================] - 3s 322ms/step - loss: 0.9705 - acc: 0.7366 - val_loss: 0.5428 - val_acc: 0.8125\n",
      "Epoch 102/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.8622 - acc: 0.7470Epoch 1/200\n",
      "9/9 [==============================] - 3s 341ms/step - loss: 0.8252 - acc: 0.7599 - val_loss: 0.5413 - val_acc: 0.8203\n",
      "Epoch 103/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.7997 - acc: 0.7632Epoch 1/200\n",
      "9/9 [==============================] - 3s 335ms/step - loss: 0.8024 - acc: 0.7599 - val_loss: 0.5335 - val_acc: 0.8281\n",
      "Epoch 104/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.7700 - acc: 0.7832Epoch 1/200\n",
      "9/9 [==============================] - 3s 335ms/step - loss: 0.7897 - acc: 0.7814 - val_loss: 0.5312 - val_acc: 0.8281\n",
      "Epoch 105/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.7328 - acc: 0.7834Epoch 1/200\n",
      "9/9 [==============================] - 3s 324ms/step - loss: 0.7049 - acc: 0.7903 - val_loss: 0.5258 - val_acc: 0.8281\n",
      "Epoch 106/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.7764 - acc: 0.7834Epoch 1/200\n",
      "9/9 [==============================] - 3s 334ms/step - loss: 0.7180 - acc: 0.7975 - val_loss: 0.5300 - val_acc: 0.8203\n",
      "Epoch 107/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.9043 - acc: 0.7500Epoch 1/200\n",
      "9/9 [==============================] - 3s 329ms/step - loss: 0.8886 - acc: 0.7552 - val_loss: 0.5325 - val_acc: 0.8203\n",
      "Epoch 108/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.8546 - acc: 0.7773Epoch 1/200\n",
      "9/9 [==============================] - 3s 333ms/step - loss: 0.8426 - acc: 0.7724 - val_loss: 0.5230 - val_acc: 0.8281\n",
      "Epoch 109/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.7985 - acc: 0.7672Epoch 1/200\n",
      "9/9 [==============================] - 3s 329ms/step - loss: 0.7905 - acc: 0.7616 - val_loss: 0.5130 - val_acc: 0.8516\n",
      "Epoch 110/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.7122 - acc: 0.8077Epoch 1/200\n",
      "9/9 [==============================] - 3s 315ms/step - loss: 0.7191 - acc: 0.7939 - val_loss: 0.5100 - val_acc: 0.8516\n",
      "Epoch 111/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.6943 - acc: 0.7915Epoch 1/200\n",
      "9/9 [==============================] - 3s 328ms/step - loss: 0.7125 - acc: 0.7903 - val_loss: 0.5138 - val_acc: 0.8438\n",
      "Epoch 112/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.7992 - acc: 0.7832Epoch 1/200\n",
      "9/9 [==============================] - 3s 327ms/step - loss: 0.8038 - acc: 0.7832 - val_loss: 0.5191 - val_acc: 0.8359\n",
      "Epoch 113/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.6763 - acc: 0.7832Epoch 1/200\n",
      "9/9 [==============================] - 3s 357ms/step - loss: 0.6867 - acc: 0.7778 - val_loss: 0.5236 - val_acc: 0.8359\n",
      "Epoch 114/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.6919 - acc: 0.8057Epoch 1/200\n",
      "9/9 [==============================] - 3s 329ms/step - loss: 0.7171 - acc: 0.8011 - val_loss: 0.5253 - val_acc: 0.8359\n",
      "Epoch 115/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.7161 - acc: 0.7731Epoch 1/200\n",
      "9/9 [==============================] - 3s 331ms/step - loss: 0.7029 - acc: 0.7833 - val_loss: 0.5188 - val_acc: 0.8359\n",
      "Epoch 116/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.5981 - acc: 0.8008Epoch 1/200\n",
      "9/9 [==============================] - 3s 328ms/step - loss: 0.5924 - acc: 0.8056 - val_loss: 0.5125 - val_acc: 0.8359\n",
      "Epoch 117/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.7581 - acc: 0.7935Epoch 1/200\n",
      "9/9 [==============================] - 3s 331ms/step - loss: 0.7451 - acc: 0.7975 - val_loss: 0.5153 - val_acc: 0.8359\n",
      "Epoch 118/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.5750 - acc: 0.8117Epoch 1/200\n",
      "9/9 [==============================] - 3s 330ms/step - loss: 0.5593 - acc: 0.8154 - val_loss: 0.5172 - val_acc: 0.8359\n",
      "Epoch 119/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.6075 - acc: 0.8279Epoch 1/200\n",
      "9/9 [==============================] - 3s 319ms/step - loss: 0.6045 - acc: 0.8172 - val_loss: 0.5135 - val_acc: 0.8359\n",
      "Epoch 120/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.7650 - acc: 0.7996Epoch 1/200\n",
      "9/9 [==============================] - 3s 321ms/step - loss: 0.7509 - acc: 0.8011 - val_loss: 0.5066 - val_acc: 0.8359\n",
      "Epoch 121/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.7762 - acc: 0.7996Epoch 1/200\n",
      "9/9 [==============================] - 3s 329ms/step - loss: 0.7386 - acc: 0.8082 - val_loss: 0.4935 - val_acc: 0.8359\n",
      "Epoch 122/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.6659 - acc: 0.8117Epoch 1/200\n",
      "9/9 [==============================] - 3s 344ms/step - loss: 0.6440 - acc: 0.8154 - val_loss: 0.4880 - val_acc: 0.8438\n",
      "Epoch 123/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.6408 - acc: 0.8077Epoch 1/200\n",
      "9/9 [==============================] - 3s 332ms/step - loss: 0.6264 - acc: 0.8065 - val_loss: 0.4881 - val_acc: 0.8359\n",
      "Epoch 124/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.5087 - acc: 0.8300Epoch 1/200\n",
      "9/9 [==============================] - 3s 341ms/step - loss: 0.5137 - acc: 0.8315 - val_loss: 0.4889 - val_acc: 0.8438\n",
      "Epoch 125/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.7212 - acc: 0.8036Epoch 1/200\n",
      "9/9 [==============================] - 3s 329ms/step - loss: 0.7118 - acc: 0.8029 - val_loss: 0.4860 - val_acc: 0.8438\n",
      "Epoch 126/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.6486 - acc: 0.8027Epoch 1/200\n",
      "9/9 [==============================] - 3s 320ms/step - loss: 0.6444 - acc: 0.8038 - val_loss: 0.4823 - val_acc: 0.8438\n",
      "Epoch 127/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.6204 - acc: 0.8138Epoch 1/200\n",
      "9/9 [==============================] - 3s 328ms/step - loss: 0.6291 - acc: 0.8136 - val_loss: 0.4802 - val_acc: 0.8438\n",
      "Epoch 128/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.6488 - acc: 0.8214Epoch 1/200\n",
      "9/9 [==============================] - 3s 313ms/step - loss: 0.6271 - acc: 0.8222 - val_loss: 0.4842 - val_acc: 0.8516\n",
      "Epoch 129/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.6568 - acc: 0.8184Epoch 1/200\n",
      "9/9 [==============================] - 3s 328ms/step - loss: 0.6605 - acc: 0.8118 - val_loss: 0.4860 - val_acc: 0.8438\n",
      "Epoch 130/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.6423 - acc: 0.7930Epoch 1/200\n",
      "9/9 [==============================] - 3s 338ms/step - loss: 0.6335 - acc: 0.7986 - val_loss: 0.4858 - val_acc: 0.8438\n",
      "Epoch 131/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.5327 - acc: 0.8340Epoch 1/200\n",
      "9/9 [==============================] - 3s 332ms/step - loss: 0.5231 - acc: 0.8387 - val_loss: 0.4863 - val_acc: 0.8438\n",
      "Epoch 132/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.5441 - acc: 0.8340Epoch 1/200\n",
      "9/9 [==============================] - 3s 352ms/step - loss: 0.5360 - acc: 0.8387 - val_loss: 0.4876 - val_acc: 0.8438\n",
      "Epoch 133/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.5935 - acc: 0.8242Epoch 1/200\n",
      "9/9 [==============================] - 3s 339ms/step - loss: 0.6092 - acc: 0.8142 - val_loss: 0.4885 - val_acc: 0.8516\n",
      "Epoch 134/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.4633 - acc: 0.8462Epoch 1/200\n",
      "9/9 [==============================] - 3s 322ms/step - loss: 0.5205 - acc: 0.8370 - val_loss: 0.4896 - val_acc: 0.8516\n",
      "Epoch 135/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.6635 - acc: 0.8036Epoch 1/200\n",
      "9/9 [==============================] - 3s 319ms/step - loss: 0.6263 - acc: 0.8190 - val_loss: 0.4899 - val_acc: 0.8516\n",
      "Epoch 136/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.5595 - acc: 0.8398Epoch 1/200\n",
      "9/9 [==============================] - 3s 335ms/step - loss: 0.5671 - acc: 0.8333 - val_loss: 0.4865 - val_acc: 0.8438\n",
      "Epoch 137/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.5711 - acc: 0.8219Epoch 1/200\n",
      "9/9 [==============================] - 3s 341ms/step - loss: 0.5569 - acc: 0.8297 - val_loss: 0.4882 - val_acc: 0.8438\n",
      "Epoch 138/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.7331 - acc: 0.7874Epoch 1/200\n",
      "9/9 [==============================] - 3s 320ms/step - loss: 0.7190 - acc: 0.7903 - val_loss: 0.4955 - val_acc: 0.8438\n",
      "Epoch 139/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.4620 - acc: 0.8482Epoch 1/200\n",
      "9/9 [==============================] - 3s 341ms/step - loss: 0.4802 - acc: 0.8441 - val_loss: 0.5054 - val_acc: 0.8516\n",
      "Epoch 140/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.5474 - acc: 0.8281Epoch 1/200\n",
      "9/9 [==============================] - 3s 308ms/step - loss: 0.5581 - acc: 0.8280 - val_loss: 0.5025 - val_acc: 0.8438\n",
      "Epoch 141/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.5398 - acc: 0.8360Epoch 1/200\n",
      "9/9 [==============================] - 3s 348ms/step - loss: 0.5584 - acc: 0.8297 - val_loss: 0.5003 - val_acc: 0.8438\n",
      "Epoch 142/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.5930 - acc: 0.8300Epoch 1/200\n",
      "9/9 [==============================] - 3s 343ms/step - loss: 0.5758 - acc: 0.8297 - val_loss: 0.5034 - val_acc: 0.8516\n",
      "Epoch 143/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.5163 - acc: 0.8398Epoch 1/200\n",
      "9/9 [==============================] - 3s 342ms/step - loss: 0.5259 - acc: 0.8351 - val_loss: 0.5004 - val_acc: 0.8516\n",
      "Epoch 144/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.5429 - acc: 0.8381Epoch 1/200\n",
      "9/9 [==============================] - 3s 342ms/step - loss: 0.5431 - acc: 0.8351 - val_loss: 0.4932 - val_acc: 0.8516\n",
      "Epoch 145/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.6089 - acc: 0.8193Epoch 1/200\n",
      "9/9 [==============================] - 3s 320ms/step - loss: 0.6046 - acc: 0.8222 - val_loss: 0.4864 - val_acc: 0.8438\n",
      "Epoch 146/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.5163 - acc: 0.8198Epoch 1/200\n",
      "9/9 [==============================] - 3s 320ms/step - loss: 0.5247 - acc: 0.8190 - val_loss: 0.4818 - val_acc: 0.8438\n",
      "Epoch 147/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.5301 - acc: 0.8239Epoch 1/200\n",
      "9/9 [==============================] - 3s 323ms/step - loss: 0.5581 - acc: 0.8208 - val_loss: 0.4814 - val_acc: 0.8438\n",
      "Epoch 148/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.5588 - acc: 0.8398Epoch 1/200\n",
      "9/9 [==============================] - 3s 328ms/step - loss: 0.5491 - acc: 0.8420 - val_loss: 0.4834 - val_acc: 0.8438\n",
      "Epoch 149/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.4834 - acc: 0.8502Epoch 1/200\n",
      "9/9 [==============================] - 3s 330ms/step - loss: 0.4713 - acc: 0.8548 - val_loss: 0.4847 - val_acc: 0.8438\n",
      "Epoch 150/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/9 [=========================>....] - ETA: 0s - loss: 0.5203 - acc: 0.8401Epoch 1/200\n",
      "9/9 [==============================] - 3s 325ms/step - loss: 0.5156 - acc: 0.8405 - val_loss: 0.4845 - val_acc: 0.8438\n",
      "Epoch 151/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.4735 - acc: 0.8462Epoch 1/200\n",
      "9/9 [==============================] - 3s 345ms/step - loss: 0.4461 - acc: 0.8513 - val_loss: 0.4848 - val_acc: 0.8438\n",
      "Epoch 152/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.4514 - acc: 0.8664Epoch 1/200\n",
      "9/9 [==============================] - 3s 347ms/step - loss: 0.4556 - acc: 0.8674 - val_loss: 0.4821 - val_acc: 0.8438\n",
      "Epoch 153/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.5364 - acc: 0.8360Epoch 1/200\n",
      "9/9 [==============================] - 3s 343ms/step - loss: 0.5219 - acc: 0.8369 - val_loss: 0.4775 - val_acc: 0.8516\n",
      "Epoch 154/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.4332 - acc: 0.8684Epoch 1/200\n",
      "9/9 [==============================] - 3s 317ms/step - loss: 0.4152 - acc: 0.8710 - val_loss: 0.4733 - val_acc: 0.8438\n",
      "Epoch 155/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.6095 - acc: 0.8164Epoch 1/200\n",
      "9/9 [==============================] - 3s 344ms/step - loss: 0.6035 - acc: 0.8177 - val_loss: 0.4732 - val_acc: 0.8594\n",
      "Epoch 156/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.5215 - acc: 0.8445Epoch 1/200\n",
      "9/9 [==============================] - 3s 309ms/step - loss: 0.5309 - acc: 0.8407 - val_loss: 0.4778 - val_acc: 0.8516\n",
      "Epoch 157/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.5386 - acc: 0.8535Epoch 1/200\n",
      "9/9 [==============================] - 3s 331ms/step - loss: 0.5372 - acc: 0.8524 - val_loss: 0.4743 - val_acc: 0.8594\n",
      "Epoch 158/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.5317 - acc: 0.8441Epoch 1/200\n",
      "9/9 [==============================] - 3s 332ms/step - loss: 0.5249 - acc: 0.8441 - val_loss: 0.4761 - val_acc: 0.8516\n",
      "Epoch 159/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.5293 - acc: 0.8401Epoch 1/200\n",
      "9/9 [==============================] - 3s 319ms/step - loss: 0.5377 - acc: 0.8351 - val_loss: 0.4751 - val_acc: 0.8594\n",
      "Epoch 160/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.4567 - acc: 0.8462Epoch 1/200\n",
      "9/9 [==============================] - 3s 322ms/step - loss: 0.4588 - acc: 0.8477 - val_loss: 0.4751 - val_acc: 0.8594\n",
      "Epoch 161/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.5884 - acc: 0.8320Epoch 1/200\n",
      "9/9 [==============================] - 3s 320ms/step - loss: 0.5987 - acc: 0.8280 - val_loss: 0.4728 - val_acc: 0.8594\n",
      "Epoch 162/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.5221 - acc: 0.8482Epoch 1/200\n",
      "9/9 [==============================] - 3s 347ms/step - loss: 0.5332 - acc: 0.8423 - val_loss: 0.4706 - val_acc: 0.8672\n",
      "Epoch 163/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.4481 - acc: 0.8644Epoch 1/200\n",
      "9/9 [==============================] - 3s 329ms/step - loss: 0.4755 - acc: 0.8602 - val_loss: 0.4732 - val_acc: 0.8594\n",
      "Epoch 164/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.4843 - acc: 0.8583Epoch 1/200\n",
      "9/9 [==============================] - 3s 324ms/step - loss: 0.4774 - acc: 0.8602 - val_loss: 0.4774 - val_acc: 0.8594\n",
      "Epoch 165/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.4374 - acc: 0.8613Epoch 1/200\n",
      "9/9 [==============================] - 3s 349ms/step - loss: 0.4734 - acc: 0.8542 - val_loss: 0.4761 - val_acc: 0.8594\n",
      "Epoch 166/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.4508 - acc: 0.8502Epoch 1/200\n",
      "9/9 [==============================] - 3s 305ms/step - loss: 0.4901 - acc: 0.8426 - val_loss: 0.4749 - val_acc: 0.8594\n",
      "Epoch 167/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.4266 - acc: 0.8496Epoch 1/200\n",
      "9/9 [==============================] - 3s 337ms/step - loss: 0.4513 - acc: 0.8472 - val_loss: 0.4709 - val_acc: 0.8594\n",
      "Epoch 168/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.4792 - acc: 0.8340Epoch 1/200\n",
      "9/9 [==============================] - 3s 342ms/step - loss: 0.4755 - acc: 0.8387 - val_loss: 0.4690 - val_acc: 0.8672\n",
      "Epoch 169/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.4625 - acc: 0.8623Epoch 1/200\n",
      "9/9 [==============================] - 3s 307ms/step - loss: 0.4932 - acc: 0.8574 - val_loss: 0.4675 - val_acc: 0.8672\n",
      "Epoch 170/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.4715 - acc: 0.8633Epoch 1/200\n",
      "9/9 [==============================] - 3s 353ms/step - loss: 0.4710 - acc: 0.8628 - val_loss: 0.4643 - val_acc: 0.8594\n",
      "Epoch 171/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.3907 - acc: 0.8664Epoch 1/200\n",
      "9/9 [==============================] - 3s 320ms/step - loss: 0.3706 - acc: 0.8763 - val_loss: 0.4663 - val_acc: 0.8594\n",
      "Epoch 172/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.4310 - acc: 0.8745Epoch 1/200\n",
      "9/9 [==============================] - 3s 337ms/step - loss: 0.4115 - acc: 0.8763 - val_loss: 0.4624 - val_acc: 0.8594\n",
      "Epoch 173/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.4803 - acc: 0.8502Epoch 1/200\n",
      "9/9 [==============================] - 3s 324ms/step - loss: 0.4560 - acc: 0.8530 - val_loss: 0.4592 - val_acc: 0.8594\n",
      "Epoch 174/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.3398 - acc: 0.8809Epoch 1/200\n",
      "9/9 [==============================] - 3s 349ms/step - loss: 0.3443 - acc: 0.8767 - val_loss: 0.4634 - val_acc: 0.8594\n",
      "Epoch 175/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.4186 - acc: 0.8603Epoch 1/200\n",
      "9/9 [==============================] - 3s 323ms/step - loss: 0.4197 - acc: 0.8674 - val_loss: 0.4658 - val_acc: 0.8672\n",
      "Epoch 176/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.3583 - acc: 0.8866Epoch 1/200\n",
      "9/9 [==============================] - 3s 316ms/step - loss: 0.3653 - acc: 0.8833 - val_loss: 0.4654 - val_acc: 0.8750\n",
      "Epoch 177/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.4717 - acc: 0.8594Epoch 1/200\n",
      "9/9 [==============================] - 3s 337ms/step - loss: 0.4739 - acc: 0.8594 - val_loss: 0.4582 - val_acc: 0.8672\n",
      "Epoch 178/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.3968 - acc: 0.8684Epoch 1/200\n",
      "9/9 [==============================] - 3s 319ms/step - loss: 0.3819 - acc: 0.8728 - val_loss: 0.4471 - val_acc: 0.8672\n",
      "Epoch 179/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.3347 - acc: 0.8846Epoch 1/200\n",
      "9/9 [==============================] - 3s 331ms/step - loss: 0.3251 - acc: 0.8835 - val_loss: 0.4452 - val_acc: 0.8672\n",
      "Epoch 180/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.4207 - acc: 0.8633Epoch 1/200\n",
      "9/9 [==============================] - 3s 329ms/step - loss: 0.4172 - acc: 0.8638 - val_loss: 0.4500 - val_acc: 0.8594\n",
      "Epoch 181/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.4855 - acc: 0.8594Epoch 1/200\n",
      "9/9 [==============================] - 3s 334ms/step - loss: 0.4590 - acc: 0.8646 - val_loss: 0.4595 - val_acc: 0.8594\n",
      "Epoch 182/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.4487 - acc: 0.8676Epoch 1/200\n",
      "9/9 [==============================] - 3s 330ms/step - loss: 0.4678 - acc: 0.8630 - val_loss: 0.4654 - val_acc: 0.8516\n",
      "Epoch 183/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.4648 - acc: 0.8535Epoch 1/200\n",
      "9/9 [==============================] - 3s 335ms/step - loss: 0.4979 - acc: 0.8507 - val_loss: 0.4596 - val_acc: 0.8516\n",
      "Epoch 184/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.4020 - acc: 0.8785Epoch 1/200\n",
      "9/9 [==============================] - 3s 332ms/step - loss: 0.3912 - acc: 0.8746 - val_loss: 0.4546 - val_acc: 0.8594\n",
      "Epoch 185/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.4733 - acc: 0.8508Epoch 1/200\n",
      "9/9 [==============================] - 3s 321ms/step - loss: 0.4502 - acc: 0.8556 - val_loss: 0.4485 - val_acc: 0.8672\n",
      "Epoch 186/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.4482 - acc: 0.8633Epoch 1/200\n",
      "9/9 [==============================] - 3s 340ms/step - loss: 0.4354 - acc: 0.8628 - val_loss: 0.4438 - val_acc: 0.8672\n",
      "Epoch 187/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.3692 - acc: 0.8826Epoch 1/200\n",
      "9/9 [==============================] - 3s 321ms/step - loss: 0.3846 - acc: 0.8781 - val_loss: 0.4454 - val_acc: 0.8672\n",
      "Epoch 188/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.4143 - acc: 0.8684Epoch 1/200\n",
      "9/9 [==============================] - 3s 332ms/step - loss: 0.4254 - acc: 0.8620 - val_loss: 0.4517 - val_acc: 0.8672\n",
      "Epoch 189/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.3904 - acc: 0.8826Epoch 1/200\n",
      "9/9 [==============================] - 3s 330ms/step - loss: 0.3786 - acc: 0.8889 - val_loss: 0.4515 - val_acc: 0.8672\n",
      "Epoch 190/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.4175 - acc: 0.8704Epoch 1/200\n",
      "9/9 [==============================] - 3s 311ms/step - loss: 0.4469 - acc: 0.8584 - val_loss: 0.4569 - val_acc: 0.8750\n",
      "Epoch 191/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.4186 - acc: 0.8613Epoch 1/200\n",
      "9/9 [==============================] - 3s 341ms/step - loss: 0.4127 - acc: 0.8646 - val_loss: 0.4633 - val_acc: 0.8672\n",
      "Epoch 192/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.4252 - acc: 0.8462Epoch 1/200\n",
      "9/9 [==============================] - 3s 343ms/step - loss: 0.4212 - acc: 0.8513 - val_loss: 0.4658 - val_acc: 0.8750\n",
      "Epoch 193/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.3885 - acc: 0.8866Epoch 1/200\n",
      "9/9 [==============================] - 3s 343ms/step - loss: 0.4023 - acc: 0.8852 - val_loss: 0.4677 - val_acc: 0.8750\n",
      "Epoch 194/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.4278 - acc: 0.8770Epoch 1/200\n",
      "9/9 [==============================] - 3s 324ms/step - loss: 0.4349 - acc: 0.8785 - val_loss: 0.4660 - val_acc: 0.8828\n",
      "Epoch 195/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.3029 - acc: 0.9139Epoch 1/200\n",
      "9/9 [==============================] - 3s 323ms/step - loss: 0.3209 - acc: 0.9037 - val_loss: 0.4658 - val_acc: 0.8828\n",
      "Epoch 196/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.3346 - acc: 0.8770Epoch 1/200\n",
      "9/9 [==============================] - 3s 322ms/step - loss: 0.3286 - acc: 0.8785 - val_loss: 0.4684 - val_acc: 0.8828\n",
      "Epoch 197/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.4003 - acc: 0.8761Epoch 1/200\n",
      "9/9 [==============================] - 3s 332ms/step - loss: 0.4168 - acc: 0.8741 - val_loss: 0.4657 - val_acc: 0.8828\n",
      "Epoch 198/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.3599 - acc: 0.8828Epoch 1/200\n",
      "9/9 [==============================] - 3s 329ms/step - loss: 0.3689 - acc: 0.8819 - val_loss: 0.4603 - val_acc: 0.8750\n",
      "Epoch 199/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.3045 - acc: 0.8927Epoch 1/200\n",
      "9/9 [==============================] - 3s 332ms/step - loss: 0.3008 - acc: 0.8961 - val_loss: 0.4632 - val_acc: 0.8750\n",
      "Epoch 200/200\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.4415 - acc: 0.8785Epoch 1/200\n",
      "9/9 [==============================] - 3s 323ms/step - loss: 0.4419 - acc: 0.8799 - val_loss: 0.4647 - val_acc: 0.8750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff20c3e0a50>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "augmenting data\n",
    "\"\"\"\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    rotation_range=30,\n",
    "    shear_range=30,\n",
    "\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.3,\n",
    "\n",
    "    brightness_range=(0.75, 1.0),\n",
    "    channel_shift_range=75.0,\n",
    "\n",
    "    horizontal_flip=True)\n",
    "\n",
    "train_batches = train_datagen.flow_from_directory(\n",
    "    DATASET_PATH + \"/train\",\n",
    "    target_size=TARGET_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "#     interpolation='bicubic',\n",
    "    class_mode='categorical',\n",
    "    shuffle=True)\n",
    "\n",
    "valid_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "valid_batches = valid_datagen.flow_from_directory(\n",
    "    DATASET_PATH + \"/valid\",\n",
    "    target_size=TARGET_SIZE, \n",
    "    batch_size=BATCH_SIZE,\n",
    "#     interpolation='bicubic',\n",
    "    class_mode='categorical',\n",
    "    shuffle=True)\n",
    "\n",
    "NUM_CLASSES = len(train_batches.class_indices.items())\n",
    "\n",
    "\"\"\"\n",
    "training nnet\n",
    "\"\"\"\n",
    "# https://github.com/jkjung-avt/keras-cats-dogs-tutorial/blob/master/train_resnet50.py\n",
    "\n",
    "#train net\n",
    "# build our classifier model based on pre-trained ResNet50:\n",
    "# 1. we don't include the top (fully connected) layers of ResNet50\n",
    "# 2. we add a DropOut layer followed by a Dense (fully connected)\n",
    "#    layer which generates softmax class score for each class\n",
    "# 3. we compile the final model using an Adam optimizer, with a\n",
    "#    low learning rate (since we are 'fine-tuning')\n",
    "net = ResNet50(include_top=False, weights='imagenet', input_tensor=None,\n",
    "               input_shape=(TARGET_SIZE[0],TARGET_SIZE[1],3))\n",
    "x = net.output\n",
    "x = Flatten()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output_layer = Dense(NUM_CLASSES, activation='softmax', name='softmax')(x)\n",
    "net_final = Model(inputs=net.input, outputs=output_layer)\n",
    "\n",
    "\"\"\"\n",
    "#freeze layers?\n",
    "for layer in net_final.layers[:FREEZE_LAYERS]:\n",
    "    layer.trainable = False\n",
    "for layer in net_final.layers[FREEZE_LAYERS:]:\n",
    "    layer.trainable = True\n",
    "\"\"\"\n",
    "\n",
    "net_final.compile(optimizer=Adam(lr=1e-5),\n",
    "                  loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(net_final.summary())\n",
    "\n",
    "# train the model\n",
    "net_final.fit_generator(train_batches,\n",
    "                        steps_per_epoch = train_batches.samples // BATCH_SIZE,\n",
    "                        validation_data = valid_batches,\n",
    "                        validation_steps = valid_batches.samples // BATCH_SIZE,\n",
    "                        epochs = NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at: saved_models/8799_1575782539_highangle_allclass_RES50.h5\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "fill this in\n",
    "\"\"\"\n",
    "acc = 8799\n",
    "\n",
    "import time, os\n",
    "MODELS_PATH = \"saved_models\"\n",
    "if not os.path.exists(MODELS_PATH):\n",
    "    os.mkdir(MODELS_PATH)\n",
    "    \n",
    "model_filename = \"{}_{}_highangle_allclass_RES50.h5\".format(acc, round(time.time()))\n",
    "filepath = os.path.join(MODELS_PATH, model_filename)\n",
    "net_final.save(filepath)\n",
    "print(\"Model saved at:\", filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
